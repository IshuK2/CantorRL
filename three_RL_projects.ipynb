{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce8bd1c-c5f2-46cd-92db-69a83363cae3",
   "metadata": {},
   "source": [
    "# 3 Reinforcement Learning Algorithms Everybody Should Know\n",
    "\n",
    "<img src=\"./media/rlearning_2.png\" width=600>\n",
    "\n",
    "Reinforcement learning is a type of machine learning where an *agent* learns to make decisions by interacting with an environment. The main components of reinforcement learning are:\n",
    "\n",
    "1. **Agent**: The entity that learns and makes decisions.\n",
    "2. **Environment**: The world in which the agent operates.\n",
    "3. **State**: The current situation of the agent in the environment.\n",
    "4. **Action**: A decision made by the agent.\n",
    "5. **Reward**: Positive or negative feedback from the environment based on the agent's action.\n",
    "6. **Policy**: The strategy the agent uses to determine its actions.\n",
    "\n",
    "The goal of reinforcement learning is for the agent to learn an optimal policy that maximizes cumulative rewards over time. This is achieved through a process of trial and error, where the agent explores the environment, takes actions, and learns from the consequences (rewards) of those actions.\n",
    "\n",
    "## In This Notebook we'll cover 3 Major Algorithms and Implementations Including...\n",
    "\n",
    "### Q-Learning\n",
    "\n",
    "<img src=\"./media/qtable.png\" width=325>\n",
    "\n",
    "### Deep Q-Network (DQN)\n",
    "\n",
    "<img src=\"./media/DQN.png\" width=325>\n",
    "\n",
    "### Proximal Policy Optimization (PPO)\n",
    "\n",
    "<img src=\"./media/PPO.png\" width=325>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9635ef8-63fa-40d5-aa5d-8049470ce643",
   "metadata": {},
   "source": [
    "---\n",
    "# Q-Learning\n",
    "\n",
    "Q-learning is a model-free RL algorithm used to find an optimal action-selection policy for finite Markov Decision Processes (MDPs).\n",
    "\n",
    "This means that the agent does not need an understanding of the environment (model-free) to learn and act, and keeps track of the optimal actions to take in given states that it observes- best used in environments with a finite set of steps, states, and actions to take (finite MDP). Q in this case stands for \"Quality\"\n",
    "\n",
    "## Core Concepts\n",
    "\n",
    "**Value-based learning**: Q-learning estimates the value of taking actions in states to inform decision-making.\n",
    "\n",
    "**Exploration vs. Exploitation**: The agent must balance trying new actions (exploration) with using known good actions (exploitation).\n",
    "\n",
    "**State-action pairs**: Instead of learning values for states alone, Q-learning considers the value of specific actions in each state.\n",
    "\n",
    "**Iterative improvement**: Q-values are continuously updated based on new experiences, gradually improving the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee9a1f0-1409-46f6-8521-af906e6977d4",
   "metadata": {},
   "source": [
    "### Code Example - Reach the Goal\n",
    "\n",
    "For our Q-Learning example we will teach our agent to reach the goal of this 3x3 Grid. The end goal will be to create a `Q-Table` that stores all of the `Q-Values` learned through the RL training.\n",
    "\n",
    "**Q-value**: Represents the expected utility of taking a specific action in a given state, this is the value kept that determines how 'useful' an action is given the current state.  \n",
    "**Q-table**: A lookup table storing Q-values for all state-action pairs, the main brains behind your agent at the end of training.\n",
    "\n",
    "This will show, at each tile in our grid, what the best action is to take. A higher Q-Value for an action in a given state action pair represents the optimal learned action.\n",
    "\n",
    "<img src=\"./media/qgame_1.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192adc43-5162-46f8-bd77-7eedf0282704",
   "metadata": {},
   "source": [
    "#### Setting Up the Environment and Actions\n",
    "\n",
    "Our environment will be represented as x,y coordinates and some helper functions to check where we are in the grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2fc0e8c-db4c-4694-88d8-b26c2a2f59c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple, List\n",
    "\n",
    "# Define the grid world\n",
    "GRID_SIZE = 3\n",
    "START = (0, 0)\n",
    "GOAL = (2, 2)\n",
    "OBSTACLE = (1, 1)\n",
    "\n",
    "# Define actions\n",
    "ACTIONS = [\n",
    "    (-1, 0),  # up\n",
    "    (0, 1),   # right\n",
    "    (1, 0),   # down\n",
    "    (0, -1)   # left\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac10e78-7d76-4d9e-9adf-639915b703ae",
   "metadata": {},
   "source": [
    "`is_valid_state` ensures state is within grid boundaries, returns `True` if valid, `False` if not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6af23468-3743-47c8-84ea-94bedd5c9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_state(state: Tuple[int, int]) -> bool:\n",
    "    return (0 <= state[0] < GRID_SIZE and \n",
    "            0 <= state[1] < GRID_SIZE and \n",
    "            state != OBSTACLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5473ca-a319-4d83-9de6-aa52706fd454",
   "metadata": {},
   "source": [
    "`get_next_state` adds the action taken to the current state and checks if it's a valid \n",
    "- If valid, returns new position\n",
    "- If invalid, returns same position (agent doesn't move)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "440ff245-2ca2-4984-9ce4-3fa97b90d30b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_state(state: Tuple[int, int], action: Tuple[int, int]) -> Tuple[int, int]:\n",
    "    next_state = (state[0] + action[0], state[1] + action[1])\n",
    "    return next_state if is_valid_state(next_state) else state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b0bac1-a348-478b-8f78-197dc680d973",
   "metadata": {},
   "source": [
    "#### Defining Q-Learning parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d1b048-bc96-4186-ba5f-fa983b606be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 0.3\n",
    "ALPHA = 0.3\n",
    "GAMMA = 0.99\n",
    "EPISODES = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469203fb-115f-4f1d-b000-16099be75e57",
   "metadata": {},
   "source": [
    "1. **EPSILON (ε) = 0.3**\n",
    "   - This is the exploration rate.\n",
    "   - It determines the probability of the agent choosing a random action instead of the best known action.\n",
    "   - With EPSILON = 0.3, the agent will explore (choose a random action) 30% of the time and exploit (choose the best known action) 70% of the time.\n",
    "   - This balance helps the agent discover new potentially better paths while also utilizing what it has learned.  \n",
    "\n",
    "2. **ALPHA (α) = 0.3**\n",
    "   - This is the learning rate.\n",
    "   - It determines how much the agent values new information compared to existing information.\n",
    "   - With ALPHA = 0.3, the agent will incorporate 30% of the new information and retain 70% of the old information when updating Q-values.\n",
    "   - A higher value makes the agent adapt more quickly to new information, while a lower value makes it more conservative.\n",
    "\n",
    "3. **GAMMA (γ) = 0.99**\n",
    "   - This is the discount factor.\n",
    "   - It determines how much the agent values future rewards compared to immediate rewards.\n",
    "   - With GAMMA = 0.99, future rewards are valued at 99% of their actual value.\n",
    "   - A value close to 1 (like 0.99) means the agent cares almost as much about long-term rewards as short-term rewards.\n",
    "   - This encourages the agent to consider long-term consequences of its actions.\n",
    "\n",
    "4. **EPISODES = 10000**\n",
    "   - This is the number of training episodes.\n",
    "   - Each episode represents one complete run through the environment, from the start state to a terminal state (either reaching the goal or a maximum number of steps).\n",
    "   - 10,000 episodes give the agent many opportunities to explore the environment and learn the optimal policy.\n",
    "\n",
    "**EPSILON** ensures a balance between exploration and exploitation.  \n",
    "**ALPHA** controls how quickly the agent adapts to new information.  \n",
    "**GAMMA** encourages the agent to consider long-term rewards.  \n",
    "**EPISODES** determines how many chances the agent gets to learn.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff060cc-08a5-483a-a79f-e4cf38e607eb",
   "metadata": {},
   "source": [
    "#### Creating Agent Rewards\n",
    "\n",
    "In RL, rewards are scalar feedback signals that indicate how well the agent is doing at a given time step. It's essentially the agent's way of understanding whether its actions are good or bad in terms of achieving its goal. Generally, reinforcement learning algorithms attempt to learn to maximize the reward gained.\n",
    "\n",
    "Our primary objective here is to reach the goal tile, so we want to setup some reward that rewards doing that, and penalizes making incorrect moves like hitting walls or obstacles. As a secondary objective, we also want to find the most efficient path.\n",
    "\n",
    "\n",
    "In `get_reward` we return a positive reward of 100 if we have reached the goal, and a penalty (negative reward) of -10 for hitting a wall or obstacle for our primary objective. To ensure an efficient path, we have a small penalty for each step taken.\n",
    "\n",
    "The specific values (100, -10, -1) are somewhat arbitrary and can be adjusted. What's important to focus on here is the relative differences between these values. Reaching the goal should be much more rewarding than the penalty for a single step, and hitting an obstacle should be more punishing than taking an extra step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e3b6736-ef5b-4d65-94b1-0f354afe475c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reward(state: Tuple[int, int], next_state: Tuple[int, int]) -> int:\n",
    "    if next_state == GOAL:\n",
    "        return 100\n",
    "    elif next_state == OBSTACLE or next_state == state:  # Penalize hitting walls or obstacle\n",
    "        return -10\n",
    "    else:\n",
    "        return -1  # Small penalty for each step to encourage shortest path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c942fe3f-8435-4481-bc39-ffef30306fbd",
   "metadata": {},
   "source": [
    "#### Choosing an Action - Exploration vs Exploitation\n",
    "\n",
    "In `choose_action`, our first parameter **ε** is implemented!  \n",
    "\n",
    "The line `if random.uniform(0, 1) < EPSILON` generates a random number 0 to 1, and compares it to our epsilon of 0.3. This implements the **ε-greedy strategy**: with probability EPSILON, the agent will explore; otherwise, it will exploit (or choose the best known action).\n",
    "\n",
    "If the random number is less than EPSILON, the function returns a random action from the ACTIONS list, and if the random number is greater than or equal to EPSILON, the function chooses the action with the highest Q-value for the current state.\n",
    "- Balancing exploration and exploitation ensures the agent doesn't always choose the best known action (which could lead to getting stuck in suboptimal solutions) or always choose randomly (which would prevent learning). This allows for **continuous learning**- even when the agent has learned a good policy, the occasional random actions allow it to potentially find even better solutions.\n",
    "- With an epsilon of 0.3, we will take a random action 30% of the time, and exploit the best known action 70% of the time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "278cc819-311c-4a29-a085-436b297c6f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state: Tuple[int, int], q_table: np.ndarray) -> Tuple[int, int]:\n",
    "    if random.uniform(0, 1) < EPSILON:\n",
    "        return random.choice(ACTIONS)\n",
    "    else:\n",
    "        return ACTIONS[np.argmax(q_table[state])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dca98e3-47cc-4127-a0f5-9aea13192f48",
   "metadata": {},
   "source": [
    "#### Updating Q-Value Learning & The Bellman Equation\n",
    "\n",
    "The Q-Values themselves that constitute which action in a state-action pair is the best to take are determined based on multiple factors, calculated using the Bellman Equation:\n",
    "\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- Q(s, a) `q_table[state][action_idx]`: The Q-value for taking action \\(a\\) in state \\(s\\).\n",
    "- α `ALPHA`: Learning rate, controlling how much the new information overrides the old.\n",
    "- \\(r\\) `reward`: Reward received after taking action \\(a\\).\n",
    "- γ `GAMMA`: Discount factor, determining the importance of future rewards.\n",
    "- \\(s'\\) `next_state`: The next state after taking action \\(a\\).\n",
    "- maxQ(s', a')\\) `np.max(q_table[next_state])`: Maximum expected future reward for the next state \\(s'\\).\n",
    "\n",
    "This is where the actual learning happens in Q-learning and the agent updates its understanding of the Q-value of actions in different states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0812f96e-2e7d-4a5e-b467-585894765dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_q_table(q_table: np.ndarray, state: Tuple[int, int], action: Tuple[int, int], \n",
    "                   reward: int, next_state: Tuple[int, int]) -> None:\n",
    "    action_idx = ACTIONS.index(action)\n",
    "    q_table[state][action_idx] += ALPHA * (reward + GAMMA * np.max(q_table[next_state]) - q_table[state][action_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba5208-5d3a-460a-b84e-98eb9b2accb9",
   "metadata": {},
   "source": [
    "**Gamma** as the discount factor:\n",
    "* If GAMMA = 1: Future rewards would be valued equally to immediate rewards. The agent would care just as much about rewards far in the future as it does about \n",
    "immediate rewards.\n",
    "* If GAMMA = 0: The agent would only care about immediate rewards and ignore future rewards completely.\n",
    "* With GAMMA = 0.99: Future rewards are valued at 99% of their actual value. This means the agent cares a lot about future rewards, but slightly less than immediate rewards.\n",
    "\n",
    "**ALPHA** as the learning rate:\n",
    "* If ALPHA = 1: The old Q-value would be completely replaced by the new estimate. This would result in very rapid learning, but also high volatility and potential instability.\n",
    "* If ALPHA = 0: The Q-value would never update, and no learning would occur.\n",
    "* With ALPHA = 0.3: The Q-value is updated by moving 30% of the way toward the new estimate.\n",
    "     * If the new estimate is higher than the old Q-value, the Q-value will increase, but only by 30% of the difference.\n",
    "     * If the new estimate is lower than the old Q-value, the Q-value will decrease, but only by 30% of the difference. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b42e0d-101b-4422-8b2f-802653500a25",
   "metadata": {},
   "source": [
    "### Training Script\n",
    "\n",
    "The main agent training script returns the learned Q-Table by following these steps:\n",
    "\n",
    "1. Create an empty (zeroed) table the same size as the environment, 3x3 in our example case\n",
    "2. Start the current state at the starting point, (0, 0)\n",
    "3. Choose an action to take based on the current state and the Q-Table\n",
    "4. Determine the next state based on the action chosen and the current state\n",
    "5. Calculate the reward based on the action taken\n",
    "6. Update the Q-Table for the current state-action pair based on the reward and the next state\n",
    "7. Move to the next state for the next iteration of the loop and repeat until the goal is reached\n",
    "8. Repeat this process for how many episodes chosen, 10,000\n",
    "\n",
    "Finally, the Q-Table is returned which will outline the best moves to make for the given objective!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24a454e9-1a73-4661-ae2c-921b23e11ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent() -> np.ndarray:\n",
    "    q_table = np.zeros((GRID_SIZE, GRID_SIZE, len(ACTIONS)))\n",
    "    \n",
    "    for _ in range(EPISODES):\n",
    "        state = START\n",
    "        while state != GOAL:\n",
    "            action = choose_action(state, q_table)\n",
    "            next_state = get_next_state(state, action)\n",
    "            reward = get_reward(state, next_state)\n",
    "            update_q_table(q_table, state, action, reward, next_state)\n",
    "            state = next_state\n",
    "    \n",
    "    return q_table\n",
    "\n",
    "# Train the agent\n",
    "q_table = train_agent()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0f85a2-bfb5-4e02-89c4-5430c9a7b28b",
   "metadata": {},
   "source": [
    "#### Visualization\n",
    "\n",
    "<img src=\"./media/qgame_2.png\" width=300>\n",
    "\n",
    "Now that we have a final Q-Table representing our trained agent, we can visualize what was learned during the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8777aa9f-893b-4742-91b7-4d4fcb91e549",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_q_table_as_grid(q_table: np.ndarray) -> None:\n",
    "    \"\"\"Visualize the Q-table as a grid with all action values for each state.\"\"\"\n",
    "    action_symbols = ['^', '>', 'v', '<']\n",
    "    \n",
    "    print(\"\\nDetailed Q-table Grid:\")\n",
    "    \n",
    "    # Header\n",
    "    header = \"   |\" + \"|\".join(f\"   ({i},{j})   \" for i in range(GRID_SIZE) for j in range(GRID_SIZE)) + \"|\"\n",
    "    print(header)\n",
    "    print(\"-\" * len(header))\n",
    "\n",
    "    for action_idx, action_symbol in enumerate(action_symbols):\n",
    "        row = f\" {action_symbol} |\"\n",
    "        for i in range(GRID_SIZE):\n",
    "            for j in range(GRID_SIZE):\n",
    "                if (i, j) == GOAL:\n",
    "                    cell = \"   GOAL    \"\n",
    "                elif (i, j) == OBSTACLE:\n",
    "                    cell = \" OBSTACLE  \"\n",
    "                else:\n",
    "                    q_value = q_table[i, j, action_idx]\n",
    "                    cell = f\" {q_value:9.2f} \"\n",
    "                row += cell + \"|\"\n",
    "        print(row)\n",
    "        print(\"-\" * len(header))\n",
    "\n",
    "def visualize_best_actions_grid(q_table: np.ndarray) -> None:\n",
    "    \"\"\"Visualize the best action and its Q-value for each state in a grid.\"\"\"\n",
    "    action_symbols = ['^', '>', 'v', '<']\n",
    "    \n",
    "    print(\"\\nBest Actions Grid:\")\n",
    "    header = \"-\" * (14 * GRID_SIZE + 1)\n",
    "    print(header)\n",
    "\n",
    "    for i in range(GRID_SIZE):\n",
    "        row = \"| \"\n",
    "        for j in range(GRID_SIZE):\n",
    "            if (i, j) == GOAL:\n",
    "                cell = \"   GOAL    \"\n",
    "            elif (i, j) == OBSTACLE:\n",
    "                cell = \" OBSTACLE  \"\n",
    "            else:\n",
    "                best_action_idx = np.argmax(q_table[i, j])\n",
    "                best_q_value = q_table[i, j, best_action_idx]\n",
    "                cell = f\"{action_symbols[best_action_idx]}:{best_q_value:7.2f}  \"\n",
    "            row += cell + \" | \"\n",
    "        print(row)\n",
    "        print(header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1381f772-1af1-40af-b6b6-2a87ac916065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Q-table Grid:\n",
      "   |   (0,0)   |   (0,1)   |   (0,2)   |   (1,0)   |   (1,1)   |   (1,2)   |   (2,0)   |   (2,1)   |   (2,2)   |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " ^ |     83.12 |     85.06 |     87.02 |     92.12 | OBSTACLE  |     96.02 |     94.06 |     89.00 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " > |     94.06 |     96.02 |     87.02 |     85.06 | OBSTACLE  |     89.00 |     98.00 |    100.00 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " v |     94.06 |     85.06 |     98.00 |     96.02 | OBSTACLE  |    100.00 |     87.02 |     89.00 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      " < |     83.12 |     92.12 |     94.06 |     85.06 | OBSTACLE  |     89.00 |     87.02 |     96.02 |   GOAL    |\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Best Actions Grid:\n",
      "-------------------------------------------\n",
      "| >:  94.06   | >:  96.02   | v:  98.00   | \n",
      "-------------------------------------------\n",
      "| v:  96.02   |  OBSTACLE   | v: 100.00   | \n",
      "-------------------------------------------\n",
      "| >:  98.00   | >: 100.00   |    GOAL     | \n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Visualize the Q-table as a grid\n",
    "visualize_q_table_as_grid(q_table)\n",
    "\n",
    "# Visualize the best actions and their Q-values in a grid\n",
    "visualize_best_actions_grid(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef90777-507d-4c30-873c-b79e8fe45bd6",
   "metadata": {},
   "source": [
    "While Q-tables are excellent for smaller, discrete state-action spaces, many real-world applications involve large or continuous state spaces. In such cases, more advanced techniques like Deep Q-Networks (DQN) or other deep reinforcement learning methods are often used, which build upon the principles of Q-learning but use neural networks to approximate the Q-function instead of a table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f89fb3f-a6ce-46f1-9262-14e95606bf07",
   "metadata": {},
   "source": [
    "---\n",
    "# Deep Q-Networks\n",
    "\n",
    "<img src=\"./media/dqn_2.png\" width=600>\n",
    "\n",
    "The main issue with Q-Learning is that it is **tabular** and **non-scalable** for more complex environments. When dealing with a limited and finite set like our grid example above, basic Q-Learning worked great since we could easily cover all possible state-action pairs, but when it comes to applying reinforcement learning to more complex environments where this is not realistic- we need to find ways to best estimate the Q-Values of a state.\n",
    "\n",
    "This is where Deep Q-Networks are introduced! Instead of having a defined lookup table, we can employ neural network estimation to learn to estimate and generalize actions at different states in an environment, being able to even learn **continuous** states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fbd240-a100-4a43-a6c8-08ab88438141",
   "metadata": {},
   "source": [
    "### Code Example - Balancing a Pole\n",
    "\n",
    "Let's look at a classic example of a more complex environment, the [CartPole-v1 example](https://www.gymlibrary.dev/environments/classic_control/cart_pole/):\n",
    "\n",
    "<img src=\"./media/cart_pole.gif\" width=300>\n",
    "\n",
    "We'll be using [gymnasium](https://gymnasium.farama.org/), a library with standardized reinforcement learning environments for testing and learning\n",
    "\n",
    "Within this environment, we have two actions for the cart:\n",
    "1. Moving Left (0)\n",
    "2. Moving Right (1)\n",
    "\n",
    "With the state observations:\n",
    "1. **Cart Position**: -4.8 to 4.8\n",
    "2. **Cart Velocity**: -inf to inf\n",
    "3. **Pole Angle**: -24° to 24°\n",
    "4. **Pole Angular Velocity**: -inf to inf\n",
    "\n",
    "And termination conditions:\n",
    "1. **Termination**: Pole Angle > ±12°\n",
    "2. **Termination**: Cart Position > ±2.4\n",
    "3. **Truncation**: Episode length > 500\n",
    "\n",
    "With the objective being to balance the pole as long as possible- thus the **reward** will be +1 per step\n",
    "\n",
    "So as we can see, an infinite combination of state-action observations, something we could definitely not tackle with traditional Q-Learning approaches!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd15e01e-b823-4f84-b7ff-458a2b0d0ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import gymnasium as gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model, layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a2a039-8558-4c86-b1cc-dc66b36ae086",
   "metadata": {},
   "source": [
    "### Creating the Environment\n",
    "\n",
    "Using gymnasium, we'll start an instance of `CartPole-v1`, we'll also grab the state and action sizes (dimensions) to help us create our network next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fba7c60-4fed-4a57-8e96-c5ec4979fd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Size:  4\n",
      "Action Size:  2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(\"State Size: \", state_size)\n",
    "print(\"Action Size: \", action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eea7cea-c3af-40b6-a167-29da571b15f6",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "<img src=\"./media/dqn_net.png\" width=1000>\n",
    "\n",
    "Our model will consist of 4 total layers:\n",
    "- It takes in 4-dimensional state information of cart position, cart velocity, pole angle, and pole angular velocity\n",
    "- Processes it through two hidden layers, each with 24 neurons\n",
    "- Outputs 2 values, representing the Q-values for the two possible actions of moving left or right\n",
    "\n",
    "This gives us a whopping *770* trainable parameters to learn the environment and refine our predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2562a1d6-9a23-4178-a69b-fc8881aee861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(Model):\n",
    "    def __init__(self, action_size, **kwargs):\n",
    "        super(DQN, self).__init__(**kwargs)\n",
    "        self.action_size = action_size\n",
    "        self.d1 = layers.Dense(24, activation='relu', name='d1')\n",
    "        self.d2 = layers.Dense(24, activation='relu', name='d2')\n",
    "        self.d3 = layers.Dense(action_size, activation='linear', name='d3')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        x = self.d2(x)\n",
    "        return self.d3(x)\n",
    "\n",
    "    # Configs for loading the saved model file later on\n",
    "    def get_config(self):\n",
    "        config = super(DQN, self).get_config()\n",
    "        config.update({\"action_size\": self.action_size})\n",
    "        return config\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        return cls(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3875ce0-78f5-4c8e-bf92-d235de6abeaa",
   "metadata": {},
   "source": [
    "### Creating the Agent\n",
    "\n",
    "Now that we're working with a more advanced approach, our agent becomes a little more complicated. Let's break it down and, and all of the different techniques being used here\n",
    "\n",
    "The first technique we'll implement is **Experience Replay**. This memory stores tuples of (state, action, reward, next_state, done), representing the agent's experiences. The DQN algorithm samples randomly from this memory during training, which helps to break the correlation between consecutive samples and stabilizes learning.\n",
    "\n",
    "The process works as such:\n",
    "1. As the agent interacts with the environment, it stores each experience tuple (state, action, reward, next_state, done) in the replay memory\n",
    "2. During training, instead of using the most recent experience, the agent randomly samples a batch of experiences from the memory.\n",
    "3. The agent then uses this batch to update its Q-network, computing the loss and performing a gradient descent step.\n",
    "\n",
    "This is beneficial because in a typical RL setting consecutive experiences are often highly correlated, so random sampling breaks these correlations, reducing the variance of updates and preventing the network from overfitting to recent experiences. As a plus each experience can be used multiple times for learning, making the learning process more data-efficient. In short, rather than updating the network after every single action, it updates based on a batch of experiences.\n",
    "\n",
    "In our Agent class that we are about to introduce, we store these replays using `remember()`, and it comes back in `replay()` where we sample a batch of replays using `minibatch = random.sample(memory, batch_size)` for training on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f46b537e-44d0-4550-bcd7-bd841a642c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay memory deque\n",
    "memory = deque(maxlen=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d14d7c2-80e5-445e-8616-b2e480b90078",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995, learning_rate=0.001):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        self.model = self._build_model()\n",
    "        self.target_model = self._build_model()\n",
    "        self.update_target_model()\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "    def _build_model(self):\n",
    "        return DQN(self.action_size)\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        q_values = self.model(np.array([state]))\n",
    "        return np.argmax(q_values[0].numpy())\n",
    "\n",
    "    def save_model(self, filepath):\n",
    "        self.model.save(filepath)\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "        # Load the saved model from the specified filepath\n",
    "        self.model = tf.keras.models.load_model(filepath, custom_objects={\"DQN\": DQN})\n",
    "        self.target_model = tf.keras.models.load_model(filepath, custom_objects={\"DQN\": DQN})\n",
    "        \n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            with tf.GradientTape() as tape:\n",
    "                q_values = self.model(np.array([state]), training=True)\n",
    "                q_value = q_values[0][action]\n",
    "\n",
    "                if done:\n",
    "                    target = reward\n",
    "                else:\n",
    "                    next_action = np.argmax(self.model(np.array([next_state]))[0].numpy())\n",
    "                    t = self.target_model(np.array([next_state]))[0][next_action]\n",
    "                    target = reward + self.gamma * t\n",
    "\n",
    "                loss = tf.reduce_mean(tf.square(target - q_value))\n",
    "\n",
    "            grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "            self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3552d28-9be2-454c-a160-8b44360c95f8",
   "metadata": {},
   "source": [
    "#### Q-Target Stabilization\n",
    "During the initialization phase, we create two networks, a `main` network and a `target_model`:\n",
    "```python\n",
    "# Main network\n",
    "self.model = self._build_model()\n",
    "# Target network\n",
    "self.target_model = self._build_model()\n",
    "self.update_target_model()\n",
    "```\n",
    "In basic Q-Learning, we used the Bellman Equation to update our Q-Values. However, with DQN, we use a neural network to approximate Q-values. This leads to a **moving target problem**:\n",
    "* The same network is used to compute both $Q(s_t, a_t)$ and $\\max_{a} Q(s_{t+1}, a)$.\n",
    "* As we update the network, both these values change, leading to instability.\n",
    "\n",
    "To address this, we introduce a separate **Target Network**, commonly referred to as the `Q-Target`:\n",
    "1.  The target network is used to compute the target Q-values: $y_t = r_t + \\gamma \\max_{a} Q_{\\text{target}}(s_{t+1}, a)$\n",
    "2.  The main network is updated towards this target: $\\text{Loss} = (y_t - Q_{\\text{main}}(s_t, a_t))^2$\n",
    "\n",
    "Now that our Q-Values are approximated instead of kept strictly in a table, we improve our accuracy by updating our neural network weights through regular model training techniques. We use the target network's output to create a stable learning target.\n",
    "\n",
    "In this context, the *target* is our estimate of the total future reward for taking a particular action in a particular state. It's essentially the value we want our estimate to move towards, or our best guess of the true value we're trying to learn.\n",
    "\n",
    "This is implemented in the training loop via:\n",
    "```python\n",
    "next_action = np.argmax(self.model(np.array([next_state]))[0].numpy())\n",
    "t = self.target_model(np.array([next_state]))[0][next_action]\n",
    "target[0][action] = reward + self.gamma * t\n",
    "```\n",
    "\n",
    "The target network is then only updated every `n` steps or episodes. This allows the targets to eventually reflect the improved knowledge of the main network, but in a more controlled, step-wise manner.\n",
    "*We will update it every 5 episodes in our training later on*\n",
    "```python\n",
    "def update_target_model(self):\n",
    "    # Copy weights from main model to target model\n",
    "    self.target_model.set_weights(self.model.get_weights())\n",
    "```\n",
    "\n",
    "Using a target network gives us the benefits of:\n",
    "- **Stability**: By keeping the target values stable for a period, it reduces the moving target problem.\n",
    "- **Reduced Correlations**: It helps break the correlation between the predicted Q-values and the target.\n",
    "- **Smoother Learning**: The periodic updates create a stepwise learning process, which can be more stable than continuous updates.\n",
    "\n",
    "**Math Time!** Here's how the Bellman equation is slightly modified to include this:  \n",
    "1. The standard Bellman equation for Q-learning is:  \n",
    "\n",
    "\n",
    "   $Q(s_t, a_t) = r_t + \\gamma \\max_{a} Q(s_{t+1}, a)$  \n",
    "   In practice, we often use an iterative update rule:  \n",
    "\n",
    "\n",
    "   $Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [r_t + \\gamma \\max_{a} Q(s_{t+1}, a) - Q(s_t, a_t)]$  \n",
    "   Where $\\alpha$ is the learning rate.  \n",
    "\n",
    "3. DQN with Target Network:  \n",
    "   The modified equation used in DQN with a target network is:  \n",
    "\n",
    "\n",
    "   $Q(s_t, a_t; \\theta) \\leftarrow Q(s_t, a_t; \\theta) + \\alpha [r_t + \\gamma \\max_{a} Q(s_{t+1}, a; \\theta^-) - Q(s_t, a_t; \\theta)]$  \n",
    "   Where:  \n",
    "   - $\\theta$ represents the parameters of the main network  \n",
    "   - $\\theta^-$ represents the parameters of the target network\n",
    "\n",
    "In our code, `self.model` corresponds to $Q(s, a; \\theta)$, and `self.target_model` corresponds to $Q(s, a; \\theta^-)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fac9a7-175d-43e2-8856-6d5db1fba102",
   "metadata": {},
   "source": [
    "#### Using the Target Model for Double DQN\n",
    "\n",
    "A very small but powerful third concept is being applied here as well within the Q-Target stabilization, **Double DQN**.\n",
    "\n",
    "In standard Q-learning and DQN training, we use the same values to both select and evaluate actions, i.e. using the model being trained to select an action for the given state, and estimating the expected future reward (which is the Q-Value of the selected action).\n",
    "\n",
    "But when we use the same function for both selection and evaluation, we can end up overestimating the true value of actions because:\n",
    "1. If our Q-value estimates have any noise (common in early training), we're more likely to select actions that have been overestimated by chance.\n",
    "2. We then use these same potentially overestimated values to evaluate how good the action is and update our Q-function, which can propagate and amplify the overestimation.\n",
    "\n",
    "This can lead to *overoptimistic* value estimates and amplifying false-positives unintentionally. Double DQN decouples the action selection from the action evaluation. The key idea is to use one set of parameters to choose the best action and another to evaluate it, which in our case will be the base model and the target model.\n",
    "\n",
    "Essentially:\n",
    "- The DQN network estimates the best action to take for the next state.\n",
    "- Then the Target network estimates the target Q-Value of taking that action in the next state.\n",
    "- Finally these are put into the bellman equation to update our Q-Value calculation/Network\n",
    "\n",
    "We implement this in the `replay()` function:\n",
    "```python\n",
    "next_action = np.argmax(self.model(np.array([next_state]))[0].numpy())\n",
    "t = self.target_model(np.array([next_state]))[0][next_action]\n",
    "target[0][action] = reward + self.gamma * t\n",
    "```\n",
    "where: \n",
    "- `np.argmax(self.model(np.array([next_state]))[0].numpy())` is using the main network (`self.model`) to select the best action.  \n",
    "- `self.target_model(np.array([next_state]))[0][next_action]` is using the target network (`self.target_model`) to evaluate the Q-value of the action selected by the main network. \n",
    "\n",
    "The benefits of this then are:\n",
    "- **Reduced Overestimation**: By using two networks, we're less likely to propagate noise as value. If one network overestimates value, the other can compensate.\n",
    "- **More Stable Learning**: The separation of selection and evaluation creates a more stable learning target.\n",
    "- **Improved Exploration**: The discrepancy between the two networks can implicitly encourage exploration.\n",
    "- **Better Generalization**: The reduced overestimation can lead to better generalization to unseen states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae5c8c9-eafa-4b5f-b3b9-eaee666b032d",
   "metadata": {},
   "source": [
    "### DQN Training\n",
    "\n",
    "Now that we have the Agent and Environment outlined and defined, let's toss some hyperparameters together and get a model trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f74a8cde-e2fd-4691-bc5d-2d6de7074c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32           # Number of samples used for each training step\n",
    "n_episodes = 500          # Total number of episodes to train on\n",
    "gamma = 0.95              # Discount factor for future rewards (0 to 1)\n",
    "epsilon = 1.0             # Initial exploration rate (1 = 100% random actions)\n",
    "epsilon_min = 0.01        # Minimum exploration rate\n",
    "epsilon_decay = 0.995     # Decay factor for epsilon after each episode\n",
    "learning_rate = 0.001     # Step size for neural network weight updates\n",
    "update_target_every = 5   # Number of episodes between target network updates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2c3989-a833-4bcc-9e87-b00cc1edf86f",
   "metadata": {},
   "source": [
    "#### Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c247c-744c-4a7b-8fb8-b0a833d6661f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "output_dir = './cartpole_model/'\n",
    "\n",
    "# Initialize the Agent\n",
    "agent = Agent(state_size, action_size, gamma=gamma, epsilon=epsilon, epsilon_min=epsilon_min, epsilon_decay=epsilon_decay, learning_rate=learning_rate)\n",
    "done = False\n",
    "\n",
    "# Main Script\n",
    "for e in range(n_episodes):\n",
    "    state = env.reset()[0]\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    for time_t in range(500):\n",
    "        action = agent.act(state[0])\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "        done = done or truncated\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state[0], action, reward, next_state[0], done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "\n",
    "        if done:\n",
    "            print(f\"Episode: {e}/{n_episodes}, Score: {time_t}, Epsilon: {agent.epsilon:.2f}\")\n",
    "            break\n",
    "\n",
    "    if len(memory) > batch_size:\n",
    "        loss = agent.replay(batch_size)\n",
    "\n",
    "    # Update epsilon\n",
    "    if agent.epsilon > agent.epsilon_min:\n",
    "        agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "    # Update target network\n",
    "    if e % update_target_every == 0:\n",
    "        agent.update_target_model()\n",
    "\n",
    "    \n",
    "    if e % 100 == 0:\n",
    "        agent.save_model(os.path.join(output_dir, f'model_{e}.keras'))\n",
    "\n",
    "agent.save_model(os.path.join(output_dir, f'model_500.keras'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00cfe58-97fe-4841-aa09-93c8fa81a791",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "We can load our model now using pure exploitation of its learned policy and see if its able to complete (get 500 reward or balance the pole) the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b46de417-60f5-4d19-817a-d113bce58cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-12 17:29:06.922 Python[30066:12571004] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 reward: 500.0\n",
      "Episode 2 reward: 500.0\n"
     ]
    }
   ],
   "source": [
    "def render_episode(agent, model_path, num_episodes=1):\n",
    "    # Load the model\n",
    "    agent.load_model(model_path)\n",
    "    \n",
    "    env = gym.make('CartPole-v1', render_mode='human')\n",
    "    for episode in range(num_episodes):\n",
    "        state, _ = env.reset()\n",
    "        state = state.reshape(1, -1)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, truncated, _ = env.step(action)\n",
    "            next_state = next_state.reshape(1, -1)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            if truncated:\n",
    "                done = True\n",
    "        print(f\"Episode {episode + 1} reward: {total_reward}\")\n",
    "    env.close()\n",
    "\n",
    "# Initializing saved model\n",
    "state_size = 4\n",
    "action_size = 2\n",
    "agent = Agent(state_size, action_size)\n",
    "agent.epsilon = 0.0  # 0.0 for pure exploitation\n",
    "\n",
    "# Load the model and render episodes\n",
    "model_path = \"./cartpole_model/DQN.keras\"\n",
    "# model_path = \"./cartpole_model/DQN_old.keras\"\n",
    "render_episode(agent, model_path, num_episodes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87926d86-4092-4f3f-a440-a4b9f92930af",
   "metadata": {},
   "source": [
    "When sampling our policy across various pole angles and pole velocities, we see a clear learned pattern.\n",
    "\n",
    "<img src=\"./media/DQN_policy_viz.png\" width=600>\n",
    "\n",
    "- **Blue** is when the cart moves left\n",
    "- **Red** is when the cart moves right\n",
    "- When the pole angle is slightly negative (leaning left) and the pole velocity is negative, the agent often chooses to move left.\n",
    "- When the pole angle and velocity are both positive, the agent typically moves right.\n",
    "\n",
    "The boundary between the two actions is smooth, which suggests that the policy is stable and the agent has learned a generalizable decision-making rule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35a2d4-08a4-4175-b326-f45f8d4cee9d",
   "metadata": {},
   "source": [
    "---\n",
    "<img src=\"./media/ppo_diagram.png\" width=600>\n",
    "\n",
    "Up until now, both Q-Learning and DQN have been examples of what's called **value-based methods**, meaning that they focus on calculating or estimating the *value* of state-action pairs. Essentially, they learn to estimate how good it is to be in a particular state or to take a specific action in a state, thus creating a value function. The policy (what action to take) is then derived from this value function, typically by choosing the action with the highest estimated value.\n",
    "\n",
    "Alternatively, we can use **policy-based methods**, which attempt to learn the policy directly without the intermediate step of calculating values. For a quick sporty example:\n",
    "- With value-based methods, you'd be telling the agent: \"Standing here with the ball is worth 5 points, but standing there is worth 2 points.\" The agent then figures out what to do based on these values.\n",
    "- With policy-based methods, you're directly teaching the agent: \"When you're here with the ball, shoot it. When you're there, pass it.\" You're learning the actions to take, not the values of states.\n",
    "\n",
    "Both approaches aim to maximize the expected return, but policy-based methods can have advantages in certain scenarios, such as handling continuous action spaces or maintaining more stable performance in some environments.\n",
    "\n",
    "To introduce this concept and highlight our third method, we'll take a look at **Proximal Policy Optimization (PPO)**. PPO is a policy-based method that uses gradient ascent to directly optimize the policy, aiming to find the best set of actions to take in different states."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4840504-cef3-4895-8ce4-76c62abc1479",
   "metadata": {},
   "source": [
    "# Proximal Policy Optimization (PPO)\n",
    "\n",
    "To Summarize:  \n",
    "\n",
    "**Q-Learning**\n",
    "- Learns a Q-value function that estimates the expected return for each state-action pair.\n",
    "- Uses a table to store Q-values for each state-action pair.\n",
    "- Policy is derived from Q-values (e.g., epsilon-greedy).\n",
    "\n",
    "**Deep Q-Network (DQN)**\n",
    "- Also learns a Q-value function, but uses a neural network to approximate it.\n",
    "- Can handle high-dimensional state spaces.\n",
    "- Policy is still derived from Q-values.\n",
    "\n",
    "And to Expand:\n",
    "\n",
    "**Proximal Policy Optimization (PPO)**\n",
    "\n",
    "<img src=\"./media/gradient_ascent.png\" width=350>\n",
    "\n",
    "- **Directly learns a policy using gradient ascent to maximize reward.**\n",
    "    - Unlike value-based methods, PPO optimizes the policy directly without the intermediate step of learning Q-values.\n",
    "    - It iteratively updates policy parameters in the direction that increases expected reward.\n",
    "- **Uses a \"surrogate\" objective function to update the policy.**\n",
    "    - This function compares the likelihood of actions under the new policy versus the old policy.\n",
    "    - It's multiplied by an estimate of the advantage (how much better an action is compared to the average).\n",
    "- **Employs a clipping mechanism to limit the size of policy updates.**\n",
    "    - This is where the term \"proximal\" comes from - it restricts how much the policy can change in a single update.\n",
    "    - This promotes stability and prevents drastic policy shifts that could lead to performance collapse.\n",
    "- **Uses an Actor-Critic architecture, including a value function estimate (Critic) to reduce variance in policy gradients.**\n",
    "    - The Actor (policy) determines action selection.\n",
    "    - The Critic (value function) aids in estimating the quality of actions and states.\n",
    "- **Works with probability distributions instead of Q-tables or deterministic values.**\n",
    "    - Unlike Q-learning or DQN which output single values for actions, PPO produces a distribution over actions.\n",
    "    - This allows for more nuanced policies, especially in complex or continuous action spaces.\n",
    "    - The probabilistic approach enables better handling of uncertainty and exploration-exploitation trade-offs.\n",
    "\n",
    "<img src=\"./media/ppo_distrib.png\" width=350>\n",
    "\n",
    "PPO aims to optimize the policy directly while ensuring that new policies don't deviate too much from old ones. This approach promotes stable learning and avoids catastrophic drops in performance, while the use of probability distributions allows for more flexible and adaptive policies in complex environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8f50e7-0f54-4325-9c92-a74003a6b7e1",
   "metadata": {},
   "source": [
    "### Code Example - Pong!\n",
    "\n",
    "Our example will be pong! Still using gymnasium but with the [Arcade Learning Environment](https://ale.farama.org/environments/pong/) addon. \n",
    "\n",
    "<img src=\"./media/ponggif.gif\" width=300>\n",
    "\n",
    "Pong has an action space of `6` possible actions:\n",
    "- 0: **NOOP** (No operation, the paddle does not move)\n",
    "- 1: **FIRE** (This is typically the action to start the game or serve the ball)\n",
    "- 2: **RIGHT** (Move the paddle right)\n",
    "- 3: **LEFT** (Move the paddle left)\n",
    "- 4: **RIGHTFIRE** (Move the paddle right and fire at the same time)\n",
    "- 5: **LEFTFIRE** (Move the paddle left and fire at the same time)\n",
    "\n",
    "And an interesting concept, our environment observation will actually be the *image*, so we need to implement some image processing methods. As a base the environments can be observed as a 210x160 pxl image with 3 color channels, and each pixel represented by a 0 to 255 value for. This will of course all be represented numerically for neural network processing.\n",
    "\n",
    "**Rewards** are calculated such that you get points for getting the ball to pass the opponent’s paddle and you lose points if the ball passes your paddle\n",
    "\n",
    "To instantiate and train this model, we'll be using the useful package [stable_baselines3](https://github.com/DLR-RM/stable-baselines3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f08a8-caa8-4b04-b205-04665fc9bc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack, VecVideoRecorder\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "import ale_py\n",
    "\n",
    "gym.register_envs(ale_py)\n",
    "\n",
    "config = {\n",
    "    \"env_name\": \"PongNoFrameskip-v4\",\n",
    "    \"num_envs\": 8,\n",
    "    \"total_timesteps\": int(10e6),\n",
    "    \"seed\": 42,    \n",
    "}\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"PPO_Pong\",\n",
    "    config = config,\n",
    "    sync_tensorboard = True,\n",
    "    monitor_gym = True,\n",
    "    save_code = True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01c499-72fb-46ae-b6fb-879403985db0",
   "metadata": {},
   "source": [
    "As we'll be using a convolutional neural network, which are designed specifically for image feature extraction, we need to do some preprocessing of the game environment, specifically rescaling and normalizing the frames, and stacking them together.\n",
    "\n",
    "#### Preprocessing Frames\n",
    "\n",
    "<img src=\"./media/preprocessing.png\" width=800>\n",
    "\n",
    "The preprocessing step follows:\n",
    "1. The raw frame is in RGB format (210x160x3). We convert it to greyscale which reduces the color channels from 3 to 1, simplifying the input.\n",
    "2. The grayscale image is resized to 84x84 pixels\n",
    "3. The pixel values are normalized by dividing by 255.0 which scales all pixel values to be between 0 and 1\n",
    "\n",
    "This process simplifies the data input for our neural network while maintaining the main features allowing us to still learn from this representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddec628-4176-4766-af90-b74b7c738f12",
   "metadata": {},
   "source": [
    "Reference output:  \n",
    "<img src=\"./media/preprocessed_frames.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809983b0-9417-4201-a28a-febd59461d9b",
   "metadata": {},
   "source": [
    "#### Stack Frames\n",
    "\n",
    "<img src=\"./media/frame_stacking.png\" width=500>\n",
    "\n",
    "Instead of using a single frame as input, we stack multiple consecutive frames to allow the network to infer motion and direction of the ball and paddles.\n",
    "\n",
    "The frame stacking follows:\n",
    "1. Fill a deque with 4 copies of the initial preprocessed frame.\n",
    "2. For each subsequent step, we preprocess the new frame and add it to the deque.\n",
    "3. Stack these 4 frames along a new axis, creating a 84x84x4 tensor.\n",
    "\n",
    "Adding this dimensionality allows us to:\n",
    "1. **Capture motion**: By having multiple consecutive frames, the network can infer the direction and speed of moving objects (like the ball).\n",
    "2. **Deal with partial observability**: A single frame might not contain all necessary information (e.g., the ball might be temporarily hidden behind a paddle).\n",
    "3. **Provide Temporal context**: It provides a short history of the game state\n",
    "\n",
    "This is all a good input for Convolutional Neural Networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1f45c3-1104-4d4b-87a3-f3fff5c41707",
   "metadata": {},
   "source": [
    "Our initial state can kind of be visualized then like:\n",
    "\n",
    "<img src=\"./media/stacked_preproc.png\" width=300>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a27def-bac1-40c4-859d-20a3c01c22a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.10.1+6a7e0ae)\n",
      "[Powered by Stella]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Action Space:  6\n"
     ]
    }
   ],
   "source": [
    "# Creating the Environment & Loading Frame Stacking\n",
    "env = make_atari_env(config[\"env_name\"], n_envs=config[\"num_envs\"], seed=config[\"seed\"]) #PongNoFrameskip-v4\n",
    "\n",
    "print(\"Environment Action Space: \", env.action_space.n)\n",
    "\n",
    "# Frame-stacking with 4 frames\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4739df29-7797-46b7-8cbb-62021d61e681",
   "metadata": {},
   "source": [
    "#### PPO Model Architecture\n",
    "\n",
    "<img src=./media/ppo_model_arch.png width=600>\n",
    "\n",
    "This architecture has:\n",
    "\n",
    "- Three convolutional layers to process the visual input.\n",
    "- A dense layer to further process the flattened convolutional output.\n",
    "- An actor head that outputs action probabilities (policy).\n",
    "- A critic head that estimates the state value.\n",
    "\n",
    "<img src=./media/ppo_layers.png width=600>\n",
    "\n",
    "Our goal for this model is to use the **objective function** to maximize the expected advantage while preventing large policy changes.\n",
    "\n",
    "**The first part is the clipped surrogate objective function:**\n",
    "\n",
    "$L(\\theta) = \\hat{\\mathbb{E}}[\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$\n",
    "\n",
    "This is the core of PPO, let's go through each component:\n",
    "\n",
    "1. $L(\\theta)$: This is the function we're trying to maximize, where $\\theta$ represents the parameters of the policy network.\n",
    "\n",
    "2. $\\hat{\\mathbb{E}}[...]$: This denotes the empirical expectation (average) over a batch of sampled data.\n",
    "\n",
    "3. $r_t(\\theta)$: This is the probability ratio between the new and old policies:\n",
    "   $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$\n",
    "   where $\\pi_\\theta$ is the current policy and $\\pi_{\\theta_{old}}$ is the old policy.\n",
    "\n",
    "4. $\\hat{A}_t$: This is the estimated advantage at time t. It measures how much better an action is compared to the average action in that state.\n",
    "\n",
    "5. $\\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)$: This function clips the ratio $r_t(\\theta)$ to be between $1-\\epsilon$ and $1+\\epsilon$, where $\\epsilon$ is a small number (often 0.1 or 0.2).\n",
    "\n",
    "6. $\\min(...)$: This takes the minimum of two terms.\n",
    "\n",
    "The full PPO objective then includes three components:\n",
    "1. The clipped surrogate objective\n",
    "2. A value function loss\n",
    "3. An entropy bonus\n",
    "\n",
    "**The complete PPO objective can be written as:**\n",
    "\n",
    "$L(\\theta) = \\hat{\\mathbb{E}}[L^{CLIP}(\\theta) - c_1 L^{VF}(\\theta) + c_2 S[\\pi_\\theta](s)]$\n",
    "\n",
    "Where:\n",
    "\n",
    "1. $L^{CLIP}(\\theta)$ is the clipped surrogate objective:  \n",
    "   $L^{CLIP}(\\theta) = \\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)$\n",
    "\n",
    "2. $L^{VF}(\\theta)$ is the value function loss:  \n",
    "   This is typically a mean squared error between the value function predictions and the actual returns. It can be written as:\n",
    "   $L^{VF}(\\theta) = (V_\\theta(s_t) - V^{target}_t)^2$\n",
    "\n",
    "3. $S[\\pi_\\theta](s)$ is the entropy of the policy:  \n",
    "   This encourages exploration by favoring policies that maintain some randomness.\n",
    "\n",
    "4. $c_1$ and $c_2$ are coefficients that balance the importance of the value function loss and entropy bonus respectively.\n",
    "\n",
    "The inclusion of the value function loss helps train the critic, improving the accuracy of the advantage estimates. The entropy term helps prevent premature convergence to deterministic policies, maintaining exploration.\n",
    "\n",
    "#### Creating the PPO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac27a09d-b84a-482c-b637-ff92aceb3f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO(policy = \"CnnPolicy\",\n",
    "            env = env,\n",
    "            batch_size = 256,\n",
    "            clip_range = 0.1,\n",
    "            ent_coef = 0.01,\n",
    "            gae_lambda = 0.9,\n",
    "            gamma = 0.99,\n",
    "            learning_rate = 2.5e-4,\n",
    "            max_grad_norm = 0.5,\n",
    "            n_epochs = 4,\n",
    "            n_steps = 128,\n",
    "            vf_coef = 0.5,\n",
    "            tensorboard_log = f\"runs\",\n",
    "            verbose=1,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d42b275-bf61-4062-b196-f7e4076591e3",
   "metadata": {},
   "source": [
    "Reiterating the core PPO objective function:\n",
    "\n",
    "$$L(\\theta) = \\hat{\\mathbb{E}}[\\min(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t)]$$\n",
    "\n",
    "1. **`clip_range = 0.1`**: \n",
    "   - Represents $\\epsilon$ in the equation\n",
    "   - Limits policy updates to prevent large changes\n",
    "\n",
    "2. **`ent_coef = 0.01`**: \n",
    "   - Adds an entropy bonus: $L(\\theta) += \\text{ent\\_coef} \\cdot S[\\pi_\\theta](s)$\n",
    "   - Encourages exploration by rewarding higher entropy policies\n",
    "\n",
    "3. **`gae_lambda = 0.9` and `gamma = 0.99`**: \n",
    "   - Used in Generalized Advantage Estimation (GAE):\n",
    "     $\\hat{A}_t = \\delta_t + (\\gamma\\lambda)\\delta_{t+1} + \\dots + (\\gamma\\lambda)^{T-t+1}\\delta_{T-1}$\n",
    "   - $\\gamma$: Discount factor for future rewards\n",
    "   - $\\lambda$: Controls bias-variance tradeoff in advantage estimation\n",
    "\n",
    "4. **`learning_rate = 2.5e-4`**: \n",
    "   - Step size for updating policy parameters $\\theta$\n",
    "\n",
    "5. **`max_grad_norm = 0.5`**: \n",
    "   - Clips gradient norm to stabilize training:\n",
    "     $\\nabla_\\theta' = \\text{clip}(\\nabla_\\theta, -0.5, 0.5)$\n",
    "\n",
    "6. **`n_epochs = 4`**: \n",
    "   - Number of optimization passes over the same data\n",
    "\n",
    "7. **`n_steps = 128`**: \n",
    "   - Number of environment steps collected before each update\n",
    "\n",
    "8. **`vf_coef = 0.5`**: \n",
    "   - Weighs the value function loss:\n",
    "     $L(\\theta) -= \\text{vf\\_coef} \\cdot (V_\\theta(s) - V_\\text{target})^2$\n",
    "\n",
    "9. **`batch_size = 256`**: \n",
    "   - Number of samples used in each optimization step\n",
    "  \n",
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a28a153-1aed-4d51-9d62-f1745b72f881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Video Recorder for WandB integration/validation recording\n",
    "env = VecVideoRecorder(env, \"videos\", record_video_trigger=lambda x: x % 100000 == 0, video_length=2000)\n",
    "\n",
    "# Main Training Script - I reccomend (and personally did) run this on a GPU. The free t4 GPU on Colab is a great option!\n",
    "model.learn(\n",
    "    total_timesteps = config[\"total_timesteps\"],\n",
    "    callback = [\n",
    "        WandbCallback(\n",
    "        gradient_save_freq = 1000,\n",
    "        model_save_path = f\"models/{run.id}\",\n",
    "        ), \n",
    "        CheckpointCallback(save_freq=10000, save_path='./pong',\n",
    "                                         name_prefix=config[\"env_name\"]),\n",
    "        ]\n",
    ")\n",
    "\n",
    "model.save(\"ppo-PongNoFrameskip-v4.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6a5dcb-b762-4de7-bba1-ec0da9e8cde3",
   "metadata": {},
   "source": [
    "#### PPO Algorithm Step-by-Step\n",
    "\n",
    "<img src=\"./media/ppo_flowchart.png\" width=600>\n",
    "\n",
    "1. **Initialize** policy parameters $\\theta_0$.\n",
    "\n",
    "2. **Collect Data:**\n",
    "   - For each environment (total `num_envs`), collect `n_steps` transitions $(s_t, a_t, r_t, s_{t+1})$ using current policy $\\pi_{\\theta_{\\text{old}}}$.\n",
    "   - Compute rewards, value estimates $V_{\\theta_{\\text{old}}}(s_t)$, and advantages $\\hat{A}_t$ using GAE.\n",
    "\n",
    "3. **Policy Update Loop** (Repeat for `n_epochs`):\n",
    "   - Shuffle data and create mini-batches of size `batch_size`.\n",
    "   - For each mini-batch:\n",
    "     a) Compute probability ratio: $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t|s_t)}$\n",
    "     \n",
    "     b) Compute losses:\n",
    "        - Clipped objective: $L_t^{\\text{CLIP}}(\\theta) = \\min( r_t(\\theta) \\hat{A}_t, \\text{clip}( r_t(\\theta), 1 - \\epsilon, 1 + \\epsilon ) \\hat{A}_t )$\n",
    "        - Value function: $L_t^{\\text{VF}}(\\theta) = ( V_\\theta(s_t) - V_{\\text{target}}(s_t) )^2$\n",
    "        - Entropy bonus: $S[\\pi_\\theta](s_t) = -\\sum_a \\pi_\\theta(a|s_t) \\log \\pi_\\theta(a|s_t)$\n",
    "     \n",
    "     c) Compute total loss:\n",
    "        $L_t(\\theta) = -L_t^{\\text{CLIP}}(\\theta) + c_{\\text{vf}} L_t^{\\text{VF}}(\\theta) - c_{\\text{entropy}} S[\\pi_\\theta](s_t)$\n",
    "     \n",
    "     d) Compute gradients: $\\nabla_\\theta L_t(\\theta)$\n",
    "     \n",
    "     e) Apply gradient clipping if $\\| \\nabla_\\theta L_t(\\theta) \\| > \\text{max\\_grad\\_norm}$\n",
    "     \n",
    "     f) Update parameters: $\\theta \\leftarrow \\theta - \\alpha \\nabla_\\theta L_t(\\theta)$, where $\\alpha$ is the learning rate\n",
    "\n",
    "**Key Components:**\n",
    "- **Clipping:** Limits policy updates to stabilize learning.\n",
    "- **Value Function:** Estimates state values to compute advantages.\n",
    "- **Entropy Bonus:** Encourages exploration by rewarding policy randomness.\n",
    "- **Gradient Clipping:** Prevents extreme parameter updates.\n",
    "\n",
    "**Note:** The negative sign before $L_t^{\\text{CLIP}}(\\theta)$ in the total loss is because we're minimizing the loss (equivalent to maximizing the PPO objective)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e5f2e6-d73b-401a-a69a-040c6a3b238d",
   "metadata": {},
   "source": [
    "#### Evaluation\n",
    "\n",
    "Let's test out the trained PPO model and see how it goes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d8b211b6-bd68-45f8-9caf-4917aaf81889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "# model = PPO.load(\"./ppo_models/pong_ppo_early.zip\")\n",
    "model = PPO.load(\"./ppo_models/pong_ppo_best.zip\")\n",
    "\n",
    "# Create and wrap the environment\n",
    "env = make_atari_env(\"PongNoFrameskip-v4\", n_envs=1, seed=0)\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "# Run the model\n",
    "obs = env.reset()\n",
    "for _ in range(5000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render(\"human\")\n",
    "\n",
    "    if dones:\n",
    "        obs = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db033e44-6cbc-43ba-bca1-97f27b9dc2e8",
   "metadata": {},
   "source": [
    "<img src=\"./media/action_prob_ppo.png\" width=600>\n",
    "\n",
    "The PPO model has learned to **output a probability distribution** over actions for any given state, instead of directly estimating action values like in **Q-learning** or **DQN**.\n",
    "The **action probabilities** shown in the plot represent how likely the agent (PPO model) is to take each of the possible actions based on the current observation (state) in the Pong game.\n",
    "\n",
    "1. **Action 0 (NOOP)**: **Probability ~ 0.032**: The agent assigns a **low probability** (~3.2%) to taking no action. This suggests the agent doesn't consider staying still as a good option in the current state.\n",
    "   \n",
    "2. **Action 1 (FIRE)**: **Probability ~ 0.14**: The agent gives **~14% probability** to the \"FIRE\" action, which in Pong is typically used to serve the ball or start the game. This suggests that the agent considers this action but isn't confident that it's the best option.\n",
    "\n",
    "3. **Action 2 (RIGHT)**: **Probability ~ 0.13**: Moving the paddle to the right has a **13% probability**, indicating the agent is considering this as a valid option but isn't strongly inclined to move right in this specific state.\n",
    "\n",
    "4. **Action 3 (LEFT)**: **Probability ~ 0.42**: The agent assigns the **highest probability** (42%) to moving the paddle to the left. This means the agent is quite confident that moving left is the best action to take in this particular game state.\n",
    "\n",
    "5. **Action 4 (RIGHTFIRE)**: **Probability ~ 0.086**: The combination of moving right and firing has a lower probability (~8.6%). The agent considers this action but doesn't view it as the most beneficial option in the current state.\n",
    "\n",
    "6. **Action 5 (LEFTFIRE)**: **Probability ~ 0.19**: Moving left and firing has a **moderate probability** (~19%). This shows that the agent considers this as a possible action, but it gives higher priority to moving left without firing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572346d4-20d9-407c-bbfd-ce7c363e1dc1",
   "metadata": {},
   "source": [
    "---\n",
    "# Closing Thoughts & LLM Tie Back\n",
    "\n",
    "<img src=\"./media/llm_ppo.png\" width=800>\n",
    "\n",
    "Reinforcement learning (RL) is the key to iteratively improving large language models (LLMs), primarily through **Reinforcement Learning from Human Feedback (RLHF)**. As an overview the process follows:\n",
    "\n",
    "1. **Pre-training and Supervised Fine-Tuning**: LLMs are first pre-trained on large datasets to learn language patterns. They are then fine-tuned using supervised learning with labeled/human-curated input-output examples.\n",
    "   \n",
    "2. **Human Feedback**: Humans rank multiple responses from the model based on quality, correctness, and helpfulness. This feedback is used to train a **reward model** that predicts the quality of future outputs.\n",
    "\n",
    "<img src=\"./media/chatgpt_preference.png\" width=600>\n",
    "\n",
    "3. **Reinforcement Learning Fine-Tuning**: The language model is fine-tuned using RL algorithms (like **PPO**), where the model generates responses, and the reward model scores them. The RL algorithm adjusts the model to maximize these scores, aligning it with human preferences.\n",
    "\n",
    "**Goals**:\n",
    "- **Human Alignment**: RLHF helps models align better with human values and generate more helpful, accurate, and safe responses.\n",
    "- **Improved Quality**: It enhances response quality by optimizing for human preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774e50df-7742-457d-be7b-7b0eb3bc6c52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
